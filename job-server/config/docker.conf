# Template for a Spark Job Server configuration file
# When deployed these settings are loaded when job server starts
#
# Spark Cluster / Job Server configuration
spark {
  # spark.master will be passed to each job's JobContext
  master = "local[2]"

  # spark web UI port
  webUrlPort = 8080

  # Default # of CPUs for jobs to use for Spark standalone cluster
  job-number-cpus = 2

  jobserver {
    port = 8090
    bind-address = "0.0.0.0"

    # If true, a separate JVM is forked for each Spark context
    context-per-jvm = true

    # Number of job results to keep per JobResultActor/context
    job-result-cache-size = 5000

    jobdao = spark.jobserver.io.JobFileDAO

    filedao {
      rootdir = /tmp/spark-jobserver/filedao/data
    }

    datadao {
      # storage directory for files that are uploaded to the server
      # via POST/data commands
      rootdir = /tmp/spark-jobserver/upload
    }

    # To load up job jars on startup, place them here,
    # with the app name as the key and the path to the jar as the value
    # job-jar-paths {
    #   test = ../job-server-tests/target/scala-2.10/job-server-tests_2.10-0.5.3-SNAPSHOT.jar
    # }

    sqldao {
      # Slick database driver, full classpath
      slick-driver = scala.slick.driver.H2Driver

      # JDBC driver, full classpath
      jdbc-driver = org.h2.Driver

      # Directory where default H2 driver stores its data. Only needed for H2.
      rootdir = /tmp/spark-jobserver/sqldao/data

      # Full JDBC URL / init string, along with username and password.  Sorry, needs to match above.
      # Substitutions may be used to launch job-server, but leave it out here in the default or tests won't pass
      jdbc {
        url = "jdbc:h2:file:/tmp/spark-jobserver/sqldao/data/h2-db"
        user = ""
        password = ""
      }

      # DB connection pool settings
      dbcp {
        maxactive = 20
        maxidle = 10
        initialsize = 10
      }
    }

    # The ask pattern timeout for Api
    short-timeout = 3 s

    # Time out for job server to wait while creating contexts
    context-creation-timeout = 60 s

    # Time out for job server to wait while creating named objects
    named-object-creation-timeout = 60 s

    # Number of jobs that can be run simultaneously per context
    # If not set, defaults to number of cores on machine where jobserver is running
    max-jobs-per-context = 1

    # in yarn deployment, time out for job server to wait while creating contexts
    yarn-context-creation-timeout = 40 s

    # spark broadcst factory in yarn deployment
    # Versions prior to 1.1.0, spark default broadcast factory is org.apache.spark.broadcast.HttpBroadcastFactory.
    # Can't start multiple sparkContexts in the same JVM with HttpBroadcastFactory.
    yarn-broadcast-factory = org.apache.spark.broadcast.TorrentBroadcastFactory
  }

  # predefined Spark contexts
  # contexts {
  #   my-low-latency-context {
  #     num-cpu-cores = 1           # Number of cores to allocate.  Required.
  #     memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, 1G, etc.
  #   }
  #   # define additional contexts here
  # }

  # universal context configuration.  These settings can be overridden, see README.md
  context-settings {
    num-cpu-cores = 2           # Number of cores to allocate.  Required.
    memory-per-node = 1G        # Executor memory per node, -Xmx style eg 512m, #1G, etc.

    # in case spark distribution should be accessed from HDFS (as opposed to being installed on every mesos slave)
    # spark.executor.uri = "hdfs://namenode:8020/apps/spark/spark.tgz"

    # uris of jars to be loaded into the classpath for this context. Uris is a string list, or a string separated by commas ','
    dependent-jar-uris = ["file:///usr/local/jars/predict_list_pull_spark-assembly-0.1.jar"]
    
    # fix for spark bug SPARK-3923 https://issues.apache.org/jira/browse/SPARK-3923
    spark.akka.heartbeat.interval = 100

    # If you wish to pass any settings directly to the sparkConf as-is, add them here in passthrough,
    # such as hadoop connection settings that don't use the "spark." prefix
    passthrough {
      es.nodes = "127.0.0.1:9200"
      es.net.http.auth.user = ""
      es.net.http.auth.pass = ""
      mongo.host = "127.0.0.1:27017"
      mongo.user = ""
      mongo.pass = ""
      amqp.host = "127.0.0.1"
      amqp.port = "5672"
      amqp.vhost = "/"
      amqp.user = ""
      amqp.pass = ""

      spray.can.server {
        # uncomment the next line for making this HTTPS
        # ssl-encryption = on
        idle-timeout = 20 s
        request-timeout = 15 s
        pipelining-limit = 2 # for maximum performance (prevents StopReading / ResumeReading messages to the IOBridge)
        # Needed for HTTP/1.0 requests with missing Host headers
        default-host-header = "spray.io:8765"
        parsing.max-content-length = 30m
      }
    }
  }

  # This needs to match SPARK_HOME for cluster SparkContexts to be created successfully
  home = "/usr/local/spark"
}

# Flyway locations to scan recursively for migrations
flyway.locations = "db/h2/migration"

akka {
  # Use SLF4J/logback for deployed environment logging
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    warn-about-java-serializer-usage = off
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
      send-buffer-size = 512000b
      receive-buffer-size = 512000b
      # This controls the maximum message size, including job results, that can be sent
      maximum-frame-size = 10 MiB
    }
  }

  cluster {
    auto-down-unreachable-after = 20s
    metrics.enabled = off
    failure-detector.acceptable-heartbeat-pause = 3s
    failure-detector.threshold = 12.0
  }
}

deploy {
  manager-start-cmd = "./manager_start.sh"
}

# Note that you can use this file to define settings not only for job server,
# but for your Spark jobs as well.  Spark job configuration merges with this configuration file as defaults.
